# A Reconfigurable Hardware Library for Robot Scene Perception
>Yanqi Liu


## Contribution

### Background
Perceiving the position and orientation of objects (i.e., pose estimation)
is a crucial prerequisite for robots acting within their
natural environment.

While neural
networks are considered the state-of-art techniques for object pose
estimation, they require significant hardware resources and manual
labeling efforts for training.

Recent works have explored custom hardware acceleration of
sampling based pose estimation algorithms through FPGAs.  However, these prior works have two main
limitations: 𝑖) they consider pose estimation assuming rigid-body
objects, and 𝑖𝑖) the hardware implementation is fixed to a specific
algorithm and cannot be reconfigured to better match design or
object constraints for a specific scenario.

### Solution
Our hardware
accelerator implements Nonparametric Belief Propagation (NBP)
to infer the belief distribution of articulated object poses.

>For 6 DoF pose estimation, the observation is the RGBD data
from the camera sensor.
>

### Result
Our approach
is on average, 26X more energy efficient than a high-end
GPU and 11X faster than an embedded low-power GPU implementation.

We present a Monte-Carlo Perception Library
generated from high-level synthesis to enable reconfigurable hardware
designs on FPGA fabrics that are better tuned to user-specified
scene, resource, and performance constraints.

We take a similar approach of using HLS
tools to develop a novel hardware library for object pose estimation,
which fills the need for a domain-specific acceleration library in
robotic pose estimation.

## Reflection
1. It is for sampling-based pose estimation, and for articulated pose estimation， which is much different from solid-body estimation.
2. This library maybe can be used in later articulated pose estimation.



![](https://github.com/XingzhenCHEN/PaperReview/blob/main/Week3/Hardware%20Library/xxx.png)
